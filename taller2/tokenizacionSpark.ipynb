{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacion de dependencias\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de dependencias\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del Spark Session\n",
    "spark = SparkSession.builder.appName('TokenizacionSpark').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Punto 1\n",
    "## Se busca construir el BoW mas optimizado del archivo solicitado\n",
    "## Eliminando stop words, tokenizando, haciendo stemming y lemmatizacion\n",
    "\n",
    "# Lectura del archivo\n",
    "file = spark.read.text('./data/0704.3504.pdf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Tokenizacion\n",
    "tokenization = Tokenizer(inputCol='value', outputCol='tokens')\n",
    "\n",
    "file = file.select(trim(col('value')).alias('value'))\n",
    "\n",
    "tokens = tokenization.transform(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Eliminacion de caracteres no alfabeticos\n",
    "def noAlphaRemove_udf(x):\n",
    "    newTokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in x]\n",
    "    newTokens = [w.lower() for w in x if len(w) > 1]\n",
    "    newTokens = [w for w in x if w.isalpha()]\n",
    "    return newTokens\n",
    "noAlphaRemove = udf(lambda s: noAlphaRemove_udf(s), ArrayType(StringType()))\n",
    "\n",
    "tokens = tokens.select(noAlphaRemove(col('tokens')).alias('tokens'))\n",
    "tokens = tokens.where(size(col('tokens'))>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remocion de stopwords\n",
    "stopWordRemover=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "tokens = stopWordRemover.transform(tokens)\n",
    "tokens = tokens.select(col('refined_tokens').alias('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution\n",
    "tokens = tokens.withColumn('token_count', size(col('tokens')))\n",
    "tokens.orderBy(rand()).show(10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}