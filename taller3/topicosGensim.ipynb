{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 3\n",
    "\n",
    "**Autor:** Juan Pablo Gaviria\n",
    "\n",
    "## Deteccion de Topicos con genSim\n",
    "\n",
    "Se realizará la detección de Topicos con genSim y LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "import snowballstemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import ascii_lowercase\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis\n",
    "import gensim\n",
    "import nltk\n",
    "from pyLDAvis import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de la informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalData = pd.read_csv('/home/jgaviria/Workspace/DataScience/datasets/metadata.csv')\n",
    "originalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparacion de la informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = originalData.drop(['sha','doi','pmcid','pubmed_id','license','mag_id','who_covidence_id','arxiv_id','pdf_json_files','pmc_json_files','url','s2_id'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea nuevo dataframe unicamente con el id y el texto a evaluar\n",
    "texto = pd.DataFrame({'id':data.cord_uid, 'texto':data.title+\" \"+data.abstract})\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan filas que no tengan ni titulo ni abstract\n",
    "texto = texto.dropna()\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparacion del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion stopwords\n",
    "# Se crea un listado de stopwords con nltk y se expande a su raiz con stem\n",
    "stemmer = snowballstemmer.EnglishStemmer()\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['may','also','zero','one','two','three','four','five','six','seven','eight','nine','ten','across','among','beside','however','yet','within']+list(ascii_lowercase))\n",
    "stopList = stemmer.stemWords(stop)\n",
    "stopList = set(stopList)\n",
    "stop = set(sorted(stop + list(stopList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de caracteres especiales\n",
    "texto['texto'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaicion, Stemming y remocion de stopwords\n",
    "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*texto['texto'].str.split(' ')))))).split(\" \"))\n",
    "texto['stemmed_text'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in texto['texto'].str.lower().str.split(' ')]\n",
    "texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan las palabras que no aparezcan por lo menos 5 veces en todos los documentos\n",
    "cuenta_minima = 5\n",
    "str_frequencies = pd.DataFrame(list(Counter(filter(None,list(itertools.chain(*texto['stemmed_text'].str.split(' '))))).items()),columns=['word','count'])\n",
    "low_frequency_words = set(str_frequencies[str_frequencies['count'] < cuenta_minima]['word'])\n",
    "texto['stemmed_text'] = [' '.join(filter(None,filter(lambda word: word not in low_frequency_words, line))) for line in texto['stemmed_text'].str.split(' ')]\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para Stemming paralelo\n",
    "def parStemming(texto):\n",
    "    return \" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', texto).split(' ')))\n",
    "\n",
    "#texto['stemmed_text'] = [\" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', next_text).split(' '))) for next_text in texto['stemmed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza Stemming\n",
    "t0 = time.time()\n",
    "pool = mp.Pool()\n",
    "texto['stemmed_text'] = pool.map(parStemming,texto['stemmed_text'])\n",
    "pool.close()\n",
    "print('Tiempo tomado: ' + str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representacion de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para tokenizacion paralela\n",
    "def parToken(texto):\n",
    "    return nltk.word_tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizacion\n",
    "t0 = time.time()\n",
    "pool = mp.Pool()\n",
    "#texto['tokens'] = texto.apply(lambda row: nltk.word_tokenize(row['stemmed_text']), axis=1)\n",
    "texto['tokens'] = pool.map(parToken, texto['stemmed_text'])\n",
    "pool.close()\n",
    "print('Tiempo tomado: ' + str(time.time()-t0))\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "diccionario = Dictionary(texto.tokens)\n",
    "print('Longitud del BoW: ' + str(len(diccionario)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de documentos vs terminos\n",
    "t0 = time.time()\n",
    "pool = mp.Pool(mp.cpu_count()-1)\n",
    "doc_term_matrix = pool.map(diccionario.doc2bow, [sentence for sentence in texto.tokens])\n",
    "pool.close()\n",
    "print('Tiempo tomado: '+str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicacion del modelo (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "lda_model = LdaMulticore(doc_term_matrix, num_topics=5, id2word=diccionario, workers=mp.cpu_count())\n",
    "print('Tiempo tomado: '+str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensim.prepare(lda_model, doc_term_matrix, diccionario, sort_topics=False)\n",
    "print('Tiempo tomado: '+str(time.time()-t0))\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "pyLDAvis.save_html(vis, 'gensimLDAOpt.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación del modelo\n",
    "\n",
    "Se aplica el modelo para clasificar todos los documentos. Esta clasificacion se utiliza para el clasificador supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para clasificacion paralela\n",
    "def clasificarDoc(documento):\n",
    "    textBow = diccionario.doc2bow(documento)\n",
    "    distribucion = lda_model.get_document_topics(textBow)\n",
    "    topico = max(distribucion, key=lambda x:x[1])[0]\n",
    "    return topico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificar todos los documentos\n",
    "t0 = time.time()\n",
    "pool = mp.Pool()\n",
    "texto['Topico'] = pool.map(clasificarDoc, texto.tokens)\n",
    "pool.close()\n",
    "print('Tiempo tomado: '+str(time.time()-t0))\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea Data Frame de salida con el id del documento y el topico\n",
    "outClasificacion = pd.DataFrame({'docId':texto.id,'topico':texto.Topico})\n",
    "outClasificacion.to_pickle('docTopicDetection.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}